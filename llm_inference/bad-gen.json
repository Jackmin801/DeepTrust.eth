{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "device": "cuda", "dtype": "float32", "engine": "transformers", "hashes": ["float32[1, 18, 4096](73728, 4096, 1)<aa928465160a2bcfd32412cd6b89f643>", "float32[1, 1, 4096](4096, 4096, 1)<f4984e739ece7968d92d3b1b612e24a7>", "float32[1, 1, 4096](4096, 4096, 1)<29373b36e19d21c349fc0e222d0dcd9a>", "float32[1, 1, 4096](4096, 4096, 1)<5aad93b667266d6d9eb7255a8434c0e6>", "float32[1, 1, 4096](4096, 4096, 1)<6503ecf7272bda825c241a7f37b9c26b>", "float32[1, 1, 4096](4096, 4096, 1)<faea240ad8b86ef2c1f5d6001ccf7314>"], "completion": [128000, 3923, 374, 18940, 1773, 1201, 939, 29510, 323, 1268, 374, 433, 5552, 311, 8537, 559, 29510, 30, 510, 35187, 933, 40, 1097, 4560], "input_tokens": 18, "generation_config": {"do_sample": true, "max_length": 24, "num_return_sequences": 1}}