{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "device": "cuda", "dtype": "float32", "engine": "transformers", "hashes": ["float32[1, 18, 4096](73728, 4096, 1)<aa928465160a2bcfd32412cd6b89f643>", "float32[1, 1, 4096](4096, 4096, 1)<fa6fc26a964eebe8e885a03a64975dc3>", "float32[1, 1, 4096](4096, 4096, 1)<b9e40dd1771b575c7948658d5c6b69ca>", "float32[1, 1, 4096](4096, 4096, 1)<f15eb116b1748a119209b27ead0733d3>", "float32[1, 1, 4096](4096, 4096, 1)<a1788b73b649b1ab7cf111520e0ad429>", "float32[1, 1, 4096](4096, 4096, 1)<63d0cb508a8ad9f5bcefa99454c0e242>"], "completion": [128000, 3923, 374, 18940, 1773, 1201, 939, 29510, 323, 1268, 374, 433, 5552, 311, 8537, 559, 29510, 30, 720, 32649, 1773, 1201, 939, 29510], "input_tokens": 18, "generation_config": {"do_sample": true, "max_length": 24, "num_return_sequences": 1}}