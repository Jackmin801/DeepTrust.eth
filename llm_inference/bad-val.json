{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "device": "cuda", "dtype": "float32", "engine": "transformers", "hashes": ["float32[1, 18, 4096](73728, 4096, 1)<aa928465160a2bcfd32412cd6b89f643>", "float32[1, 1, 4096](4096, 4096, 1)<e6fdeb0f56fa4f69aa15ceec58e5d5d0>", "float32[1, 1, 4096](4096, 4096, 1)<7bfc01e4d9f72657e1f33657037954c4>", "float32[1, 1, 4096](4096, 4096, 1)<747ff6a5eca11c002f328322d635ad69>", "float32[1, 1, 4096](4096, 4096, 1)<560efd127e41d5fd69dac3c3036cbbd4>", "float32[1, 1, 4096](4096, 4096, 1)<1b16647798b96feea6c83ea09ad32b2a>"], "completion": [128000, 3923, 374, 18940, 1773, 1201, 939, 29510, 323, 1268, 374, 433, 5552, 311, 8537, 559, 29510, 30, 3639, 374, 279, 6811, 1990, 18940], "input_tokens": 18, "generation_config": {"do_sample": true, "max_length": 24, "num_return_sequences": 1}}