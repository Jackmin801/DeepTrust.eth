{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "device": "cuda", "dtype": "float32", "engine": "transformers", "hashes": ["float32[1, 18, 4096](73728, 4096, 1)<aa928465160a2bcfd32412cd6b89f643>", "float32[1, 1, 4096](4096, 4096, 1)<1c83d87ee63d18103107d38ec2e6e3fb>", "float32[1, 1, 4096](4096, 4096, 1)<36bde5c8921867c651dbe30bca4c53ec>", "float32[1, 1, 4096](4096, 4096, 1)<956671ea77307971b7787e4009c977c9>", "float32[1, 1, 4096](4096, 4096, 1)<1e109293bde0257e2e159845b7193c66>", "float32[1, 1, 4096](4096, 4096, 1)<e6c79af5a8ecef316a213aa843978a1c>"], "completion": [128000, 3923, 374, 18940, 1773, 1201, 939, 29510, 323, 1268, 374, 433, 5552, 311, 8537, 559, 29510, 30, 510, 35187, 933, 40, 1097, 4560], "input_tokens": 18, "generation_config": {"do_sample": true, "max_length": 24, "num_return_sequences": 1}}