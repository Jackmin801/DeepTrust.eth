{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/deeptrust/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from deeptrust.models.llama.modeling_llama import LlamaForCausalLM, COMMIT_CONFIG\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"cuda\", torch_dtype=torch.float32)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/tmp/deeptrust-commits/1729411585.log')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "COMMIT_DIR = Path(\"/tmp/deeptrust-commits\")\n",
    "COMMIT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def get_commit_path_from_time():\n",
    "    return COMMIT_DIR / f\"{int(time.time())}.log\"\n",
    "\n",
    "get_commit_path_from_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing commit to /tmp/deeptrust-commits/1729411586.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", use_fast=True)\n",
    "\n",
    "input_text = \"What is proto-danksharding and how is it related to eth sharding?\"\n",
    "commit_file = get_commit_path_from_time().open(\"w\")\n",
    "print(f\"Writing commit to {commit_file.name}\")\n",
    "COMMIT_CONFIG.commit_file = commit_file\n",
    "commit_file.write(input_text)\n",
    "commit_file.write(\"\\n\")\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(input_ids.cuda(), do_sample=True, max_length=100, num_return_sequences=1)\n",
    "\n",
    "commit_file.write(tokenizer.decode(output[0]))\n",
    "\n",
    "commit_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   3923,    374,  18940,   1773,   1201,    939,  29510,    323,\n",
       "          1268,    374,    433,   5552,    311,   8537,    559,  29510,     30,\n",
       "           482,  33986,     50,   5185,    198,     36,  19041,    372,    753,\n",
       "         13707,    527,   3318,    389,    559,  29510,     11,    264,  94840,\n",
       "          6425,    430,   1436,   5376,    279,   4009,    753,   8824,    311,\n",
       "          1920,  14463,     13,  58777,   1773,   1201,    939,  29510,    374,\n",
       "           264,   1401,   3777,    315,    420,   3197,    627,   2059,  29510,\n",
       "           374,    264,  94840,   6425,    430,  18065,  45473,    279,  35046,\n",
       "          4009,   1139,   9333,     11,   9678,   5315,    315,   7954,   2663,\n",
       "         75210,     13,   9062,  53169,    690,    387,   8647,    369,   8863,\n",
       "           264,  13651,    315,    279,   4009,    753,  14463,     11,    902,\n",
       "          1288], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing commit to /tmp/deeptrust-commits/1729411593.log\n"
     ]
    }
   ],
   "source": [
    "# input_text = \"\"\"\\\n",
    "# What is proto-danksharding and how is it related to eth sharding??\n",
    "# Proto-danksharding is a proposal by Vitalik Buterin that aims to improve the scalability and security of the Ethereum network. It is a precursor to the more advanced sharding solution called Danksharding.\n",
    "# Danksharding is a sharding solution that was proposed by Vitalik Buterin in 2020. It aims to improve the scalability and security of the Ethereum network by breaking it down into\\\n",
    "# \"\"\"\n",
    "# input_text = \"\"\"\\\n",
    "# What is proto-danksharding and how is it related to eth sharding??\n",
    "# Proto-danksharding is a\n",
    "# \"\"\"\n",
    "# \n",
    "commit_file = get_commit_path_from_time().open(\"w\")\n",
    "print(f\"Writing commit to {commit_file.name}\")\n",
    "COMMIT_CONFIG.commit_file = commit_file\n",
    "COMMIT_CONFIG.input_prompt_length = 18\n",
    "# \n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "# \n",
    "# commit_file.write(tokenizer.decode(input_ids[0][:COMMIT_CONFIG.input_prompt_length], skip_special_tokens=True))\n",
    "# commit_file.write(\"\\n\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    _ = model(output[:, :24])\n",
    "\n",
    "commit_file.write(input_text)\n",
    "commit_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   3923,    374,  18940,   1773,   1201,    939,  29510,    323,\n",
       "           1268,    374,    433,   5552,    311,   8537,    559,  29510,     30]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"What is proto-danksharding and how is it related to eth sharding?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   3923,    374,  18940,   1773,   1201,    939,  29510,    323,\n",
       "           1268,    374,    433,   5552,    311,   8537,    559,  29510,     30,\n",
       "           5380,  32649,   1773,   1201,    939,  29510,    374,    264,  14050,\n",
       "            555,  55371,   1609,   2030,  85509,    430,  22262,    311,   7417,\n",
       "            279,  94840,    323,   4868,    315,    279,  35046,   4009,     13,\n",
       "           1102,    374,    264,  71261,    311,    279,    810,  11084,    559,\n",
       "          29510,   6425,   2663,  71507,    939,  29510,    627,     35,   1201,\n",
       "            939,  29510,    374,    264,    559,  29510,   6425,    430,    574,\n",
       "          11223,    555,  55371,   1609,   2030,  85509,    304,    220,   2366,\n",
       "             15,     13,   1102,  22262,    311,   7417,    279,  94840,    323,\n",
       "           4868,    315,    279,  35046,   4009,    555,  15061,    433,   1523,\n",
       "           1139]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"\"\"\\\n",
    "What is proto-danksharding and how is it related to eth sharding??\n",
    "Proto-danksharding is a proposal by Vitalik Buterin that aims to improve the scalability and security of the Ethereum network. It is a precursor to the more advanced sharding solution called Danksharding.\n",
    "Danksharding is a sharding solution that was proposed by Vitalik Buterin in 2020. It aims to improve the scalability and security of the Ethereum network by breaking it down into\\\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptrust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
